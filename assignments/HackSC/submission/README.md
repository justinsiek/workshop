# Submissions Repo

We found Nosana's GPU services to be immensely helpful in speeding up our model processing and response times. We hosted a Llama 3.2 model to recieve and process requests from our flask local backend server. It was a bit of a struggle to create a route to publicly expose the remote server, but we were able to accomplish it utilizing pyngrock. This was our first foray into using self hosted models instead of utilizing LLM APIs such as Claude and OpenAI, and we are proud of what we achieved.
